{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91dd8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5beb5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bccde58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e356f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘onegin.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/v-goncharenko/madmo-adv/55d929befa12370fc18109f5333f7cf000ea27ce/homeworks/onegin.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"onegin.txt\"\n",
    "model_name = \"ai-forever/rugpt3small_based_on_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2fa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aadbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"onegin.txt\", \"r\") as iofile:\n",
    "    text = iofile.readlines()\n",
    "\n",
    "TEXT_START = 0\n",
    "TEXT_END = -1\n",
    "text = text[TEXT_START:TEXT_END]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38196bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sonnets(text):\n",
    "    sonnets = []\n",
    "    cur_sonet = \"\"\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        if re.match(r\"^\\b[IVXLCDM]+\\b$\", line):\n",
    "            if cur_sonet:\n",
    "                sonnets.append(cur_sonet)\n",
    "                cur_sonet = \"\"\n",
    "        elif line:\n",
    "            cur_sonet += line + \"\\n\"\n",
    "    if cur_sonet:\n",
    "        sonnets.append(cur_sonet)\n",
    "    return sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0638f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«Мой дядя самых честных правил,\n",
      "Когда не в шутку занемог,\n",
      "Он уважать себя заставил\n",
      "И лучше выдумать не мог.\n",
      "Его пример другим наука;\n",
      "Но, боже мой, какая скука\n",
      "С больным сидеть и день и ночь,\n",
      "Не отходя ни шагу прочь!\n",
      "Какое низкое коварство\n",
      "Полуживого забавлять,\n",
      "Ему подушки поправлять,\n",
      "Печально подносить лекарство,\n",
      "Вздыхать и думать про себя:\n",
      "Когда же черт возьмет тебя!»\n",
      "\n",
      "Так думал молодой повеса,\n",
      "Летя в пыли на почтовых,\n",
      "Всевышней волею Зевеса\n",
      "Наследник всех своих родных. —\n",
      "Друзья Людмилы и Руслана!\n",
      "С героем моего романа\n",
      "Без предисловий, сей же час\n",
      "Позвольте познакомить вас:\n",
      "Онегин, добрый мой приятель,\n",
      "Родился на брегах Невы,\n",
      "Где, может быть, родились вы\n",
      "Или блистали, мой читатель;\n",
      "Там некогда гулял и я:\n",
      "Но вреден север для меня\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sonnets = split_into_sonnets(text)\n",
    "print(sonnets[0], sep=\"\\n\")\n",
    "print(sonnets[1], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a40203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e3240b3b2f429db13f83a38b8149dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\"text\": sonnets})\n",
    "\n",
    "max_length = 128\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9bc060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c474c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d057ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./onegin_rugpt3_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45647f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 04:14, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.849700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.773500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=2.484167046017117, metrics={'train_runtime': 255.9125, 'train_samples_per_second': 41.967, 'train_steps_per_second': 0.703, 'total_flos': 701569105920000.0, 'train_loss': 2.484167046017117, 'epoch': 30.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dad533ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я вас любил, Таня, но забыть…\n",
      "– Ах, милый… уж больно…\n",
      "К чему так много?\n",
      "К тому, что пора нам расстаться;\n",
      "Я должен ехать;\n",
      "Я вас пророните\n",
      "Сердечной дозой.\n",
      "Куда, милый мой? завтра же\n",
      "Возьму в Москве у Тани больничную койку.\n",
      "Онегин, верный мой,\n",
      "Онегин… проститься с женой собрался.\n",
      "Куда?.. где? к ней?\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, truncation=True, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "prompt = \"Я вас любил\"\n",
    "outputs = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf2f9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./onegin_rugpt3_model/tokenizer_config.json',\n",
       " './onegin_rugpt3_model/special_tokens_map.json',\n",
       " './onegin_rugpt3_model/vocab.json',\n",
       " './onegin_rugpt3_model/merges.txt',\n",
       " './onegin_rugpt3_model/added_tokens.json',\n",
       " './onegin_rugpt3_model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./onegin_rugpt3_model\")\n",
    "tokenizer.save_pretrained(\"./onegin_rugpt3_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75fae49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('/kaggle/working/onegin_model_archive.zip').unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58db08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
