{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d326e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mi\\Документы\\GitHub\\AITH-ML-Python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c535854",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/Mi/Документы/GitHub/AITH-ML-Python/models_sources/gpt_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f034d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model_name='fine_tuned_model', data_path=DATA_PATH):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенизатора.\n",
    "        Загружаем модель и токенизатор из указанного пути.\n",
    "        \"\"\"\n",
    "        model_path = Path(data_path) / model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_text(\n",
    "            self, \n",
    "            prompt: str, \n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=1.0, \n",
    "            top_k=0, \n",
    "            top_p=1.0, \n",
    "            do_sample=False,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Генерация текста на основе заданного начального текста (prompt) и параметров.\n",
    "        \n",
    "        Параметры:\n",
    "        - prompt: Текст пользователя.\n",
    "        - max_length: Максимальная длина сгенерированного текста.\n",
    "        - num_return_sequences: Количество возвращаемых последовательностей.\n",
    "        - temperature: Контролирует разнообразие вывода.\n",
    "        - top_k: Если больше 0, ограничивает количество слов для выборки только k наиболее вероятными словами.\n",
    "        - top_p: Если меньше 1.0, применяется nucleus sampling.\n",
    "        - do_sample: Если True, включает случайную выборку для увеличения разнообразия.\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        prompt_length_in_tokens = self.count_tokens(prompt)\n",
    "\n",
    "        generator = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer, truncation=True)\n",
    "        all_texts = generator(\n",
    "            prompt,\n",
    "            max_length=max_length + prompt_length_in_tokens,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=do_sample,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "            )\n",
    "        \n",
    "        # prompt_length = len(self.tokenizer.decode(encoded_input[0], skip_special_tokens=True))\n",
    "        prompt_length_in_chars = self.count_chars(prompt)\n",
    "        all_texts = [text[\"generated_text\"] for text in all_texts]\n",
    "        # trimmed_texts = [text[prompt_length:] for text in all_texts]\n",
    "        trimmed_texts = [text[prompt_length_in_chars:] for text in all_texts]\n",
    "        \n",
    "        return {\n",
    "            \"full_texts\": all_texts,\n",
    "            \"generated_texts\": trimmed_texts\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, input: str) -> int:\n",
    "        return len(self.tokenizer.encode(input, return_tensors=\"pt\")[0])\n",
    "    \n",
    "    def count_chars(self, input: str) -> int:\n",
    "        return len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dd0c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TextGenerator(\n",
    "    model_name='',\n",
    "    data_path=DATA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf59934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_texts': ['Мой дядя самых честных правил\\nЗасеребрился, а стал таким скучным.\\nОн пьет чай, ест котлету,\\nВ углу книжки читает;\\nНа всех двух дочек глядит\\nСтарик, седой, в пенсне.\\nОн пьет чай', 'Мой дядя самых честных правил,\\nОн знал толк во всем и в том,\\nЧто такое порядочный человек;\\nЗа ним все были друзья и друзья». —\\n«А ты, мой Евгений,\\nКакое благородное дело?\\nВ чем же твое благородство'], 'generated_texts': ['\\nЗасеребрился, а стал таким скучным.\\nОн пьет чай, ест котлету,\\nВ углу книжки читает;\\nНа всех двух дочек глядит\\nСтарик, седой, в пенсне.\\nОн пьет чай', ',\\nОн знал толк во всем и в том,\\nЧто такое порядочный человек;\\nЗа ним все были друзья и друзья». —\\n«А ты, мой Евгений,\\nКакое благородное дело?\\nВ чем же твое благородство']}\n",
      "\n",
      "Generated Text 1:\n",
      "Мой дядя самых честных правил\n",
      "Засеребрился, а стал таким скучным.\n",
      "Он пьет чай, ест котлету,\n",
      "В углу книжки читает;\n",
      "На всех двух дочек глядит\n",
      "Старик, седой, в пенсне.\n",
      "Он пьет чай\n",
      "\n",
      "Generated Text 2:\n",
      "Мой дядя самых честных правил,\n",
      "Он знал толк во всем и в том,\n",
      "Что такое порядочный человек;\n",
      "За ним все были друзья и друзья». —\n",
      "«А ты, мой Евгений,\n",
      "Какое благородное дело?\n",
      "В чем же твое благородство\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Мой дядя самых честных правил\"\n",
    "\n",
    "generated_texts = generator.generate_text(\n",
    "    prompt=prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    top_k=10,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(generated_texts)\n",
    "\n",
    "for i, text in enumerate(generated_texts['full_texts']):\n",
    "    print(f\"\\nGenerated Text {i+1}:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882358cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
